{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb4dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d0b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6baee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyesuaikan level logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44afd59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Type</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Date</th>\n",
       "      <th>Media</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Author</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Retweeted</th>\n",
       "      <th>Favourited</th>\n",
       "      <th>Mentions1</th>\n",
       "      <th>Sentiment1</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rt</td>\n",
       "      <td>RT @LANGKAHANIES: Jangan ada intervensi politi...</td>\n",
       "      <td>31/05/2023 23:59</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Positive</td>\n",
       "      <td>@YTrigusmintara (ManusiaBebas)</td>\n",
       "      <td>19.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>jangan ada intervensi politik penjegalan pilpr...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>jangan ada intervensi politik jegal pilpres 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rt</td>\n",
       "      <td>RT @triwul82: Sejumlah perwakilan Koalisi Peru...</td>\n",
       "      <td>31/05/2023 23:59</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@INA_NKRI (100% Indonesia ÃÂÃÂÃÂÃÂ°ÃÂ...</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sejumlah perwakilan koalisi perubahan yang men...</td>\n",
       "      <td>positif</td>\n",
       "      <td>jumlah wakil koalisi ubah yang usung anies bag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>rt</td>\n",
       "      <td>RT @ajengcute16__: Merupakan Open Legal Policy...</td>\n",
       "      <td>31/05/2023 23:59</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Positive</td>\n",
       "      <td>@sri08054 (Sri anies)</td>\n",
       "      <td>356.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>merupakan open legal policy perludem sangat be...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>rupa open legal policy perludem sangat bahaya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>rt</td>\n",
       "      <td>RT @Jatayu_45: JOKOWI HARUS MUNDUR DARI JABATA...</td>\n",
       "      <td>31/05/2023 23:59</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>@wongedan1708 (BAGong Modern)</td>\n",
       "      <td>16.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>jokowi harus mundur dari jabatan presiden kala...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>jokowi harus mundur dari jabat presiden kalau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>mention</td>\n",
       "      <td>Langkahi Presiden dan DPR, Demokrat: Bukan Wew...</td>\n",
       "      <td>31/05/2023 23:59</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Simanjunta9Nico (Nico Simanjuntak)</td>\n",
       "      <td>650.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>langkahi presiden dan dpr demokrat bukan wewen...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>langkah presiden dan dpr demokrat bukan wewena...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No     Type                                           Mentions  \\\n",
       "0   1       rt  RT @LANGKAHANIES: Jangan ada intervensi politi...   \n",
       "1   2       rt  RT @triwul82: Sejumlah perwakilan Koalisi Peru...   \n",
       "2   3       rt  RT @ajengcute16__: Merupakan Open Legal Policy...   \n",
       "3   4       rt  RT @Jatayu_45: JOKOWI HARUS MUNDUR DARI JABATA...   \n",
       "4   5  mention  Langkahi Presiden dan DPR, Demokrat: Bukan Wew...   \n",
       "\n",
       "               Date    Media Sentiment  \\\n",
       "0  31/05/2023 23:59  Twitter  Positive   \n",
       "1  31/05/2023 23:59  Twitter  Negative   \n",
       "2  31/05/2023 23:59  Twitter  Positive   \n",
       "3  31/05/2023 23:59  Twitter   Neutral   \n",
       "4  31/05/2023 23:59  Twitter  Negative   \n",
       "\n",
       "                                              Author  Followers  Retweeted  \\\n",
       "0                     @YTrigusmintara (ManusiaBebas)       19.0       64.0   \n",
       "1  @INA_NKRI (100% Indonesia ÃÂÃÂÃÂÃÂ°ÃÂ...     1250.0       83.0   \n",
       "2                              @sri08054 (Sri anies)      356.0       50.0   \n",
       "3                      @wongedan1708 (BAGong Modern)       16.0      108.0   \n",
       "4                @Simanjunta9Nico (Nico Simanjuntak)      650.0        0.0   \n",
       "\n",
       "   Favourited                                          Mentions1 Sentiment1  \\\n",
       "0         0.0  jangan ada intervensi politik penjegalan pilpr...    negatif   \n",
       "1         0.0  sejumlah perwakilan koalisi perubahan yang men...    positif   \n",
       "2         0.0  merupakan open legal policy perludem sangat be...    negatif   \n",
       "3         0.0  jokowi harus mundur dari jabatan presiden kala...    negatif   \n",
       "4         0.0  langkahi presiden dan dpr demokrat bukan wewen...    negatif   \n",
       "\n",
       "                                        stemmed_text  \n",
       "0  jangan ada intervensi politik jegal pilpres 20...  \n",
       "1  jumlah wakil koalisi ubah yang usung anies bag...  \n",
       "2  rupa open legal policy perludem sangat bahaya ...  \n",
       "3  jokowi harus mundur dari jabat presiden kalau ...  \n",
       "4  langkah presiden dan dpr demokrat bukan wewena...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Dataset\n",
    "df = pd.read_csv('Dataset_Manual_stemming.csv', encoding='latin1', delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff614f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan data teks berupa string\n",
    "df['stemmed_text'] = df['stemmed_text'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d091ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengkodekan label\n",
    "label_encoder = LabelEncoder()\n",
    "df['Sentiment1'] = label_encoder.fit_transform(df['Sentiment1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd62894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siapkan tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ce660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversikan data ke BERT InputExamples\n",
    "def convert_data_to_examples(data, data_column, label_column):\n",
    "    return data.apply(lambda x: InputExample(guid=None,\n",
    "                                             text_a=x[data_column],\n",
    "                                             text_b=None,\n",
    "                                             label=x[label_column]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf2db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah InputExamples menjadi InputFeatures\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "                                                     input_dict[\"token_type_ids\"],\n",
    "                                                     input_dict[\"attention_mask\"])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label)\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    'input_ids': f.input_ids,\n",
    "                    'attention_mask': f.attention_mask,\n",
    "                    'token_type_ids': f.token_type_ids\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64),\n",
    "        ({'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), 'token_type_ids': tf.TensorShape([None])}, tf.TensorShape([])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e61f230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 1800s 15s/step - loss: 0.8045 - accuracy: 0.6439 - val_loss: 0.5510 - val_accuracy: 0.7853\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 1743s 14s/step - loss: 0.4535 - accuracy: 0.8361 - val_loss: 0.3567 - val_accuracy: 0.8760\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 1857s 15s/step - loss: 0.3064 - accuracy: 0.8900 - val_loss: 0.3295 - val_accuracy: 0.8831\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 1782s 14s/step - loss: 0.2200 - accuracy: 0.9289 - val_loss: 0.3447 - val_accuracy: 0.8962\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 1785s 14s/step - loss: 0.1535 - accuracy: 0.9528 - val_loss: 0.3703 - val_accuracy: 0.8921\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 1778s 14s/step - loss: 0.1156 - accuracy: 0.9654 - val_loss: 0.4212 - val_accuracy: 0.8992\n",
      "Epoch 7/20\n",
      "124/124 [==============================] - 1913s 15s/step - loss: 0.1081 - accuracy: 0.9677 - val_loss: 0.4342 - val_accuracy: 0.8952\n",
      "Epoch 8/20\n",
      "124/124 [==============================] - 1747s 14s/step - loss: 0.0694 - accuracy: 0.9783 - val_loss: 0.4989 - val_accuracy: 0.8942\n",
      "Epoch 9/20\n",
      "124/124 [==============================] - 1828s 15s/step - loss: 0.0662 - accuracy: 0.9813 - val_loss: 0.4455 - val_accuracy: 0.8992\n",
      "Epoch 10/20\n",
      "124/124 [==============================] - 1771s 14s/step - loss: 0.0563 - accuracy: 0.9841 - val_loss: 0.4656 - val_accuracy: 0.9032\n",
      "Epoch 11/20\n",
      "124/124 [==============================] - 1747s 14s/step - loss: 0.0453 - accuracy: 0.9864 - val_loss: 0.4659 - val_accuracy: 0.8982\n",
      "Epoch 12/20\n",
      "124/124 [==============================] - 1732s 14s/step - loss: 0.0420 - accuracy: 0.9876 - val_loss: 0.5146 - val_accuracy: 0.8972\n",
      "Epoch 13/20\n",
      "124/124 [==============================] - 1785s 14s/step - loss: 0.0446 - accuracy: 0.9861 - val_loss: 0.4691 - val_accuracy: 0.9062\n",
      "Epoch 14/20\n",
      "124/124 [==============================] - 1763s 14s/step - loss: 0.0420 - accuracy: 0.9866 - val_loss: 0.4688 - val_accuracy: 0.9042\n",
      "Epoch 15/20\n",
      "124/124 [==============================] - 1758s 14s/step - loss: 0.0434 - accuracy: 0.9874 - val_loss: 0.5031 - val_accuracy: 0.8982\n",
      "Epoch 16/20\n",
      "124/124 [==============================] - 1796s 14s/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.5014 - val_accuracy: 0.9062\n",
      "Epoch 17/20\n",
      "124/124 [==============================] - 1784s 14s/step - loss: 0.0293 - accuracy: 0.9912 - val_loss: 0.5221 - val_accuracy: 0.8992\n",
      "Epoch 18/20\n",
      "124/124 [==============================] - 1773s 14s/step - loss: 0.0311 - accuracy: 0.9894 - val_loss: 0.5054 - val_accuracy: 0.9062\n",
      "Epoch 19/20\n",
      "124/124 [==============================] - 1791s 14s/step - loss: 0.0303 - accuracy: 0.9904 - val_loss: 0.5362 - val_accuracy: 0.8901\n",
      "Epoch 20/20\n",
      "124/124 [==============================] - 1808s 15s/step - loss: 0.0273 - accuracy: 0.9919 - val_loss: 0.5346 - val_accuracy: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 1813s 15s/step - loss: 0.7586 - accuracy: 0.6910 - val_loss: 0.5359 - val_accuracy: 0.8044\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 1725s 14s/step - loss: 0.4441 - accuracy: 0.8348 - val_loss: 0.4307 - val_accuracy: 0.8367\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 1823s 15s/step - loss: 0.2971 - accuracy: 0.8956 - val_loss: 0.3818 - val_accuracy: 0.8619\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 1782s 14s/step - loss: 0.1998 - accuracy: 0.9284 - val_loss: 0.4025 - val_accuracy: 0.8760\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 1841s 15s/step - loss: 0.1458 - accuracy: 0.9546 - val_loss: 0.4749 - val_accuracy: 0.8619\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 1774s 14s/step - loss: 0.1029 - accuracy: 0.9675 - val_loss: 0.4317 - val_accuracy: 0.8790\n",
      "Epoch 7/20\n",
      "124/124 [==============================] - 1772s 14s/step - loss: 0.0804 - accuracy: 0.9773 - val_loss: 0.5289 - val_accuracy: 0.8720\n",
      "Epoch 8/20\n",
      "124/124 [==============================] - 1765s 14s/step - loss: 0.0719 - accuracy: 0.9786 - val_loss: 0.4734 - val_accuracy: 0.8790\n",
      "Epoch 9/20\n",
      "124/124 [==============================] - 1785s 14s/step - loss: 0.0699 - accuracy: 0.9793 - val_loss: 0.4765 - val_accuracy: 0.8770\n",
      "Epoch 10/20\n",
      "124/124 [==============================] - 1760s 14s/step - loss: 0.0556 - accuracy: 0.9854 - val_loss: 0.5066 - val_accuracy: 0.8669\n",
      "Epoch 11/20\n",
      "124/124 [==============================] - 1831s 15s/step - loss: 0.0468 - accuracy: 0.9869 - val_loss: 0.5723 - val_accuracy: 0.8851\n",
      "Epoch 12/20\n",
      "124/124 [==============================] - 1752s 14s/step - loss: 0.0476 - accuracy: 0.9861 - val_loss: 0.5242 - val_accuracy: 0.8851\n",
      "Epoch 13/20\n",
      "124/124 [==============================] - 1806s 15s/step - loss: 0.0387 - accuracy: 0.9876 - val_loss: 0.5840 - val_accuracy: 0.8871\n",
      "Epoch 14/20\n",
      "124/124 [==============================] - 1746s 14s/step - loss: 0.0506 - accuracy: 0.9859 - val_loss: 0.5800 - val_accuracy: 0.8871\n",
      "Epoch 15/20\n",
      "124/124 [==============================] - 1831s 15s/step - loss: 0.0319 - accuracy: 0.9909 - val_loss: 0.5697 - val_accuracy: 0.8891\n",
      "Epoch 16/20\n",
      "124/124 [==============================] - 1791s 14s/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.5839 - val_accuracy: 0.8851\n",
      "Epoch 17/20\n",
      "124/124 [==============================] - 1771s 14s/step - loss: 0.0270 - accuracy: 0.9909 - val_loss: 0.6012 - val_accuracy: 0.8871\n",
      "Epoch 18/20\n",
      "124/124 [==============================] - 1788s 14s/step - loss: 0.0264 - accuracy: 0.9914 - val_loss: 0.6308 - val_accuracy: 0.8962\n",
      "Epoch 19/20\n",
      "124/124 [==============================] - 1816s 15s/step - loss: 0.0236 - accuracy: 0.9929 - val_loss: 0.6479 - val_accuracy: 0.8881\n",
      "Epoch 20/20\n",
      "124/124 [==============================] - 1785s 14s/step - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.6551 - val_accuracy: 0.8770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 1796s 14s/step - loss: 0.7887 - accuracy: 0.6638 - val_loss: 0.7221 - val_accuracy: 0.7409\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 1852s 15s/step - loss: 0.4336 - accuracy: 0.8467 - val_loss: 0.4502 - val_accuracy: 0.8206\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 1823s 15s/step - loss: 0.2933 - accuracy: 0.9012 - val_loss: 0.4408 - val_accuracy: 0.8387\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 1840s 15s/step - loss: 0.2197 - accuracy: 0.9276 - val_loss: 0.3938 - val_accuracy: 0.8649\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 1873s 15s/step - loss: 0.1549 - accuracy: 0.9493 - val_loss: 0.4718 - val_accuracy: 0.8569\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 1796s 14s/step - loss: 0.1216 - accuracy: 0.9632 - val_loss: 0.4280 - val_accuracy: 0.8770\n",
      "Epoch 7/20\n",
      "124/124 [==============================] - 1863s 15s/step - loss: 0.0946 - accuracy: 0.9735 - val_loss: 0.6034 - val_accuracy: 0.8508\n",
      "Epoch 8/20\n",
      "124/124 [==============================] - 1837s 15s/step - loss: 0.0815 - accuracy: 0.9778 - val_loss: 0.5400 - val_accuracy: 0.8679\n",
      "Epoch 9/20\n",
      "124/124 [==============================] - 1822s 15s/step - loss: 0.0758 - accuracy: 0.9766 - val_loss: 0.5370 - val_accuracy: 0.8629\n",
      "Epoch 10/20\n",
      "124/124 [==============================] - 1818s 15s/step - loss: 0.0604 - accuracy: 0.9818 - val_loss: 0.5800 - val_accuracy: 0.8690\n",
      "Epoch 11/20\n",
      "124/124 [==============================] - 1837s 15s/step - loss: 0.0482 - accuracy: 0.9871 - val_loss: 0.5667 - val_accuracy: 0.8780\n",
      "Epoch 12/20\n",
      "124/124 [==============================] - 1829s 15s/step - loss: 0.0425 - accuracy: 0.9861 - val_loss: 0.5745 - val_accuracy: 0.8669\n",
      "Epoch 13/20\n",
      "124/124 [==============================] - 1799s 14s/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 0.5558 - val_accuracy: 0.8790\n",
      "Epoch 14/20\n",
      "124/124 [==============================] - 1798s 14s/step - loss: 0.0466 - accuracy: 0.9856 - val_loss: 0.6457 - val_accuracy: 0.8690\n",
      "Epoch 15/20\n",
      "124/124 [==============================] - 1841s 15s/step - loss: 0.0381 - accuracy: 0.9869 - val_loss: 0.6092 - val_accuracy: 0.8700\n",
      "Epoch 16/20\n",
      "124/124 [==============================] - 1804s 15s/step - loss: 0.0381 - accuracy: 0.9876 - val_loss: 0.6221 - val_accuracy: 0.8770\n",
      "Epoch 17/20\n",
      "124/124 [==============================] - 1859s 15s/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.6355 - val_accuracy: 0.8730\n",
      "Epoch 18/20\n",
      "124/124 [==============================] - 1860s 15s/step - loss: 0.0347 - accuracy: 0.9889 - val_loss: 0.6831 - val_accuracy: 0.8780\n",
      "Epoch 19/20\n",
      "124/124 [==============================] - 1853s 15s/step - loss: 0.0290 - accuracy: 0.9907 - val_loss: 0.6998 - val_accuracy: 0.8750\n",
      "Epoch 20/20\n",
      "124/124 [==============================] - 1860s 15s/step - loss: 0.0314 - accuracy: 0.9897 - val_loss: 0.7501 - val_accuracy: 0.8669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 1832s 15s/step - loss: 0.7405 - accuracy: 0.6925 - val_loss: 0.5273 - val_accuracy: 0.8095\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 1812s 15s/step - loss: 0.4100 - accuracy: 0.8558 - val_loss: 0.4333 - val_accuracy: 0.8387\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 1828s 15s/step - loss: 0.2566 - accuracy: 0.9165 - val_loss: 0.4226 - val_accuracy: 0.8639\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 1825s 15s/step - loss: 0.1760 - accuracy: 0.9430 - val_loss: 0.4115 - val_accuracy: 0.8831\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 1835s 15s/step - loss: 0.1272 - accuracy: 0.9584 - val_loss: 0.4183 - val_accuracy: 0.8800\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 1808s 15s/step - loss: 0.0890 - accuracy: 0.9710 - val_loss: 0.4772 - val_accuracy: 0.8790\n",
      "Epoch 7/20\n",
      "124/124 [==============================] - 1847s 15s/step - loss: 0.0804 - accuracy: 0.9745 - val_loss: 0.4780 - val_accuracy: 0.8911\n",
      "Epoch 8/20\n",
      "124/124 [==============================] - 1803s 15s/step - loss: 0.0555 - accuracy: 0.9821 - val_loss: 0.4705 - val_accuracy: 0.9042\n",
      "Epoch 9/20\n",
      "124/124 [==============================] - 1861s 15s/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.5132 - val_accuracy: 0.8851\n",
      "Epoch 10/20\n",
      "124/124 [==============================] - 1933s 16s/step - loss: 0.0483 - accuracy: 0.9866 - val_loss: 0.4958 - val_accuracy: 0.8780\n",
      "Epoch 11/20\n",
      "124/124 [==============================] - 1956s 16s/step - loss: 0.0416 - accuracy: 0.9879 - val_loss: 0.4933 - val_accuracy: 0.8911\n",
      "Epoch 12/20\n",
      "124/124 [==============================] - 1889s 15s/step - loss: 0.0373 - accuracy: 0.9887 - val_loss: 0.5343 - val_accuracy: 0.8851\n",
      "Epoch 13/20\n",
      "124/124 [==============================] - 1894s 15s/step - loss: 0.0293 - accuracy: 0.9899 - val_loss: 0.5865 - val_accuracy: 0.8901\n",
      "Epoch 14/20\n",
      "124/124 [==============================] - 1843s 15s/step - loss: 0.0371 - accuracy: 0.9887 - val_loss: 0.5383 - val_accuracy: 0.8952\n",
      "Epoch 15/20\n",
      "124/124 [==============================] - 1858s 15s/step - loss: 0.0261 - accuracy: 0.9922 - val_loss: 0.5932 - val_accuracy: 0.8931\n",
      "Epoch 16/20\n",
      "124/124 [==============================] - 1882s 15s/step - loss: 0.0285 - accuracy: 0.9892 - val_loss: 0.5829 - val_accuracy: 0.8982\n",
      "Epoch 17/20\n",
      "124/124 [==============================] - 1833s 15s/step - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.6734 - val_accuracy: 0.8972\n",
      "Epoch 18/20\n",
      "124/124 [==============================] - 1872s 15s/step - loss: 0.0290 - accuracy: 0.9899 - val_loss: 0.6107 - val_accuracy: 0.8931\n",
      "Epoch 19/20\n",
      "124/124 [==============================] - 1890s 15s/step - loss: 0.0330 - accuracy: 0.9884 - val_loss: 0.6521 - val_accuracy: 0.8952\n",
      "Epoch 20/20\n",
      "124/124 [==============================] - 1868s 15s/step - loss: 0.0316 - accuracy: 0.9894 - val_loss: 0.5874 - val_accuracy: 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 1843s 15s/step - loss: 0.7508 - accuracy: 0.6872 - val_loss: 0.4941 - val_accuracy: 0.8145\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 1873s 15s/step - loss: 0.4364 - accuracy: 0.8454 - val_loss: 0.3691 - val_accuracy: 0.8679\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 1850s 15s/step - loss: 0.2872 - accuracy: 0.9017 - val_loss: 0.3540 - val_accuracy: 0.8770\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 1876s 15s/step - loss: 0.1968 - accuracy: 0.9329 - val_loss: 0.3645 - val_accuracy: 0.8881\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 1831s 15s/step - loss: 0.1332 - accuracy: 0.9569 - val_loss: 0.3632 - val_accuracy: 0.8962\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 1847s 15s/step - loss: 0.0983 - accuracy: 0.9675 - val_loss: 0.4811 - val_accuracy: 0.8871\n",
      "Epoch 7/20\n",
      "124/124 [==============================] - 1840s 15s/step - loss: 0.0915 - accuracy: 0.9685 - val_loss: 0.4386 - val_accuracy: 0.9032\n",
      "Epoch 8/20\n",
      "124/124 [==============================] - 1947s 16s/step - loss: 0.0661 - accuracy: 0.9771 - val_loss: 0.4433 - val_accuracy: 0.8962\n",
      "Epoch 9/20\n",
      "124/124 [==============================] - 1862s 15s/step - loss: 0.0491 - accuracy: 0.9836 - val_loss: 0.4636 - val_accuracy: 0.8962\n",
      "Epoch 10/20\n",
      "124/124 [==============================] - 1859s 15s/step - loss: 0.0487 - accuracy: 0.9871 - val_loss: 0.4895 - val_accuracy: 0.8962\n",
      "Epoch 11/20\n",
      "124/124 [==============================] - 1866s 15s/step - loss: 0.0383 - accuracy: 0.9881 - val_loss: 0.5476 - val_accuracy: 0.8942\n",
      "Epoch 12/20\n",
      "124/124 [==============================] - 1889s 15s/step - loss: 0.0523 - accuracy: 0.9829 - val_loss: 0.5554 - val_accuracy: 0.8780\n",
      "Epoch 13/20\n",
      "124/124 [==============================] - 1815s 15s/step - loss: 0.0406 - accuracy: 0.9871 - val_loss: 0.5894 - val_accuracy: 0.8921\n",
      "Epoch 14/20\n",
      "124/124 [==============================] - 1891s 15s/step - loss: 0.0324 - accuracy: 0.9892 - val_loss: 0.5454 - val_accuracy: 0.8962\n",
      "Epoch 15/20\n",
      "124/124 [==============================] - 1853s 15s/step - loss: 0.0295 - accuracy: 0.9894 - val_loss: 0.6212 - val_accuracy: 0.8921\n",
      "Epoch 16/20\n",
      "124/124 [==============================] - 1856s 15s/step - loss: 0.0293 - accuracy: 0.9907 - val_loss: 0.6255 - val_accuracy: 0.8921\n",
      "Epoch 17/20\n",
      "124/124 [==============================] - 1857s 15s/step - loss: 0.0342 - accuracy: 0.9892 - val_loss: 0.5689 - val_accuracy: 0.9002\n",
      "Epoch 18/20\n",
      "124/124 [==============================] - 1840s 15s/step - loss: 0.0301 - accuracy: 0.9894 - val_loss: 0.6171 - val_accuracy: 0.8992\n",
      "Epoch 19/20\n",
      "124/124 [==============================] - 1836s 15s/step - loss: 0.0352 - accuracy: 0.9887 - val_loss: 0.6553 - val_accuracy: 0.8931\n",
      "Epoch 20/20\n",
      "124/124 [==============================] - 1802s 15s/step - loss: 0.0345 - accuracy: 0.9874 - val_loss: 0.6027 - val_accuracy: 0.9002\n"
     ]
    }
   ],
   "source": [
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_list = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(df), 1):\n",
    "    train = df.iloc[train_index]\n",
    "    test = df.iloc[test_index]\n",
    "    \n",
    "    # Convert to BERT input format\n",
    "    train_InputExamples = convert_data_to_examples(train, 'stemmed_text', 'Sentiment1')\n",
    "    test_InputExamples = convert_data_to_examples(test, 'stemmed_text', 'Sentiment1')\n",
    "    \n",
    "    train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "    train_data = train_data.shuffle(100).batch(32).repeat()  # Menambahkan .repeat()\n",
    "    \n",
    "    test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n",
    "    test_data = test_data.batch(32)\n",
    "    \n",
    "    # Load BERT model\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "    \n",
    "    # Compile the model with run_eagerly=True\n",
    "    from tensorflow.keras.optimizers.legacy import Adam  \n",
    "    optimizer = Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric], run_eagerly=True)\n",
    "    \n",
    "    # Determine the number of steps per epoch and validation steps\n",
    "    train_steps_per_epoch = len(train) // 32\n",
    "    validation_steps = len(test) // 32\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_data, epochs=20, steps_per_epoch=train_steps_per_epoch, validation_data=test_data, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35fdfa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 113s 4s/step - loss: 0.6027 - accuracy: 0.9002\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(test_data, steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee2b08cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 112s 4s/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set\n",
    "predictions = model.predict(test_data, steps=validation_steps)\n",
    "y_pred = np.argmax(predictions.logits, axis=1)\n",
    "y_true = test['Sentiment1'][:len(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3d89421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 90.02%\n",
      "\n",
      "Fold 5 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.93      0.90      0.91       306\n",
      "      netral       0.93      0.86      0.89       291\n",
      "     positif       0.86      0.93      0.90       395\n",
      "\n",
      "    accuracy                           0.90       992\n",
      "   macro avg       0.91      0.90      0.90       992\n",
      "weighted avg       0.90      0.90      0.90       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracy_list)\n",
    "print(f'Average Accuracy: {average_accuracy * 100:.2f}%\\n')\n",
    "report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
    "print(f'Fold {fold} Classification Report:\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545f122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
